{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm as tqdm \n",
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "import struct\n",
    "import torch.nn as nn\n",
    "import gzip\n",
    "import array\n",
    "import math\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader ,TensorDataset\n",
    "from sklearn.model_selection import train_test_split as Split\n",
    "from tensorboard import notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    \"seed\":1314,\n",
    "    \"data_dir\":\"./data\",\n",
    "    \"train_image\":\"train-images-idx3-ubyte.gz\",\n",
    "    \"train_label\":\"train-labels-idx1-ubyte.gz\",\n",
    "    \"test_image\":\"t10k-images-idx3-ubyte.gz\",\n",
    "    \"test_label\":\"t10k-labels-idx1-ubyte.gz\",\n",
    "    \"n_epoch\":300,\n",
    "    \"warmup_steps\": 1000,\n",
    "    \"early_stop\": 1000,\n",
    "    \"save_path\": \"./models/model.ckpt\",  # save.\n",
    "    \"batch_size\":150\n",
    "}\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decode idx3 and idx1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\方伟杰\\Desktop\\DL\\CNNM\\data\n",
      "data\\t10k-images-idx3-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "print(Path(config[\"data_dir\"]).absolute())\n",
    "test_image_dir=os.path.join(Path(config[\"data_dir\"]) ,config[\"test_image\"])\n",
    "train_image_dir=os.path.join(Path(config[\"data_dir\"]) , config[\"train_image\"])\n",
    "test_label_dir=os.path.join(Path(config[\"data_dir\"]) , config[\"test_label\"])\n",
    "train_label_dir=os.path.join(Path(config[\"data_dir\"]) , config[\"train_label\"])\n",
    "print(test_image_dir)\n",
    "\n",
    "def decode_idx3_image(idx3file):\n",
    "    with gzip.open(idx3file,'rb') as f:\n",
    "        fmt_header='>iiii'  #定义读取的格式字符串大端序 一次读取4个整数(四个字节) 4*8=32bit /进行地址偏移\n",
    "        offset=f.read(16)   #第一次读取16个字节/首地址或者说是绝对地址的开头 详情见  (https://blog.csdn.net/weixin_38118997/article/details/103670901)\n",
    "        magic_number,num_image,num_rows,num_cols=struct.unpack(fmt_header,offset)\n",
    "        image_size=num_cols*num_rows\n",
    "        images = np.empty((num_image, 1,num_rows, num_cols))\n",
    "        # images=np.frombuffer(f.read(), dtype=np.uint8).reshape(num_image, 28,28)    \n",
    "        #读取数据并且转换为整数 使用下列方法需要复杂的数据转换极度麻烦\n",
    "        for i in range(num_image):\n",
    "            # print(f.read(image_size))\n",
    "            obj=array.array('b')\n",
    "            obj.frombytes(f.read(image_size))\n",
    "            image_data=np.array(obj)\n",
    "            # print(image_data)\n",
    "            image_data=image_data.reshape(1,num_rows,num_cols)\n",
    "            images[i]=image_data\n",
    "    return images\n",
    "\n",
    "\n",
    "def decode_idx1_label(idx1file):\n",
    "    with gzip.open(idx1file,'rb') as f:\n",
    "        fmt_header = '>ii'\n",
    "        offset=f.read(8)\n",
    "        magic_number,num_label=struct.unpack(fmt_header,offset)\n",
    "        labels=np.frombuffer(f.read(),dtype=np.uint8)\n",
    "    return labels    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image=decode_idx3_image(test_image_dir)\n",
    "train_image=decode_idx3_image(train_image_dir)\n",
    "test_label=decode_idx1_label(test_label_dir)  #压缩维度为(num)\n",
    "train_label=decode_idx1_label(train_label_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_image[300][0],vmin=-128, vmax=127) \n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier,self).__init__()\n",
    "        self.cnn_layer=nn.Sequential(\n",
    "            nn.Conv2d(1,16,3,1,1),\n",
    "            nn.BatchNorm2d(16,1e5),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(16,32,3,1,1),\n",
    "            nn.BatchNorm2d(32,1e5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2),(2,2),padding=0),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2),(2,2),padding=0),\n",
    "        )\n",
    "        self.fc_layer=nn.Sequential(\n",
    "            nn.Linear(64*7*7,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,10)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x=self.cnn_layer(x)\n",
    "\n",
    "        x = x.flatten(1)\n",
    "        \n",
    "        x=self.fc_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_schedule_with_warmup(\n",
    "    optimizer: Optimizer,\n",
    "    num_warmup_steps: int,\n",
    "    num_training_steps: int,\n",
    "    num_cycles: float = 0.5,\n",
    "    last_epoch: int = -1,\n",
    "):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step<num_warmup_steps:\n",
    "            return float(current_step)/float(num_warmup_steps)\n",
    "        else:\n",
    "            progress = float(current_step - num_warmup_steps) / float(\n",
    "            max(1, num_training_steps - num_warmup_steps))\n",
    "            return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_collate(data,label,batch_size=150):\n",
    "    label=label.astype(np.int64)\n",
    "    Image=torch.tensor(data)\n",
    "    label=torch.tensor(label)\n",
    "    one_hot_labels = torch.nn.functional.one_hot(label, 10)\n",
    "    Image=Image.to(torch.float)\n",
    "    label=one_hot_labels.to(torch.float)\n",
    "    data_row = TensorDataset(Image,label)\n",
    "    # print(label.shape)\n",
    "    # print(Image.shape)\n",
    "    dataload = DataLoader(data_row, batch_size=batch_size, shuffle=True)\n",
    "    return dataload\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(train_loader, valid_loader, model, config, device):\n",
    "    criterion=nn.CrossEntropyLoss()\n",
    "    optimizer=torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    writer = SummaryWriter()\n",
    "    if not os.path.isdir('./models'):\n",
    "        os.mkdir('./models') # Create directory of saving models.\n",
    "    n_epoch, best_loss, step, early_stop_count = config[\"n_epoch\"], math.inf, 0, 0\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, config[\"warmup_steps\"], n_epoch*config[\"batch_size\"])\n",
    "    for epoch in range(n_epoch):\n",
    "        model.train()\n",
    "        for x ,y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            # print(x.shape)\n",
    "            x,y=x.to(device),y.to(device)\n",
    "            L=model(x)\n",
    "            batch_loss = criterion(L, y)\n",
    "            batch_loss.backward()                     # Compute gradient(backpropagation).\n",
    "            optimizer.step() \n",
    "            scheduler.step()                   # Update parameters.\n",
    "            step += 1\n",
    "    \n",
    "        valid_loss=0\n",
    "        model.eval()\n",
    "        valid_acc=0\n",
    "        for x, y in valid_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(x)\n",
    "                valid_loss+= criterion(pred, y)\n",
    "                valid_acc+= (pred.argmax(dim=-1) == y.argmax(dim=-1)).float().mean()\n",
    "        acc=valid_acc/len(valid_loader)\n",
    "        print(f'Epoch [{epoch+1}/{n_epoch}]:Valid loss: {valid_loss:.4f}')\n",
    "        writer.add_scalar('Loss/valid', valid_loss, step)\n",
    "        writer.add_scalar('Acc/valid', acc, step)\n",
    "\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save(model.state_dict(), config[\"save_path\"]) # Save your best model\n",
    "            print('Saving model with loss {:.3f}...'.format(best_loss))\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "\n",
    "        if early_stop_count >= config[\"early_stop\"]:\n",
    "            print('\\nModel is not improving, so we halt the training session.')\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,valid_data,train_label,valid_label=Split(train_image,train_label,test_size=0.25, random_state=42,shuffle=True)\n",
    "train_loader=batch_collate(train_data,train_label,config[\"batch_size\"])\n",
    "valid_loader=batch_collate(valid_data,valid_label)\n",
    "model=Classifier().to(device)\n",
    "Train(train_loader,valid_loader,model=model,config=config,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot ACC and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 18284), started 0:01:16 ago. (Use '!kill 18284' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8b8ebf05578f778d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8b8ebf05578f778d\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=C:/Users/方伟杰/Desktop/DL/CNNM/runs/Apr02_16-55-24_DESKTOP-I9ND28M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss is :0.4779496192932129 the accuracy is :89.04973602294922%\n"
     ]
    }
   ],
   "source": [
    "test_loader=batch_collate(test_image,test_label,config[\"batch_size\"])\n",
    "model=Classifier()\n",
    "model.load_state_dict(torch.load(config[\"save_path\"]))\n",
    "model=model.to(device)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "Acc=0\n",
    "Loss=0\n",
    "for img,l in test_loader:\n",
    "    img,l=img.to(device),l.to(device)\n",
    "    with torch.no_grad():\n",
    "        predict=model(img)\n",
    "        Loss=+criterion(predict,l)\n",
    "        Acc+=(predict.argmax(dim=-1) == l.argmax(dim=-1)).float().mean()\n",
    "Acc=Acc/len(test_loader)\n",
    "print(f\"the loss is :{Loss} the accuracy is :{Acc*100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
